{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\Phosphene.AI\\sizeinvar_timesformer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Asus\\Desktop\\Phosphene.AI\\sizeinvar_timesformer\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\Desktop\\Phosphene.AI\\sizeinvar_timesformer\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Desktop\\Phosphene.AI\\timesformer\\MINTIME-Multi-Identity-size-iNvariant-TIMEsformer-for-Video-Deepfake-Detection\\examples\\fake_1_face_0.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (100.0%), showing video result...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./fake_1_face_0.avi']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_config([\"C:\\\\Users\\\\Asus\\\\Desktop\\\\Phosphene.AI\\\\timesformer\\\\MINTIME-Multi-Identity-size-iNvariant-TIMEsformer-for-Video-Deepfake-Detection\\\\examples\\\\fake_1_face_0.mp4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONFIGS & CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained/MINTIME_XC_Model_checkpoint30\"\n",
    "extractor_path = \"pretrained/MINTIME_XC_Extractor_checkpoint30\"\n",
    "video_path = \"videos/Alice Weidel  - ARD Tagesthemen Deepfake  - Compare  Version.mp4\"\n",
    "RANGE_SIZE = 5\n",
    "SIZE_EMB_DICT = [(1+i*RANGE_SIZE, (i+1)*RANGE_SIZE) if i != 0 else (0, RANGE_SIZE) for i in range(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identity_information(identity, faces):\n",
    "    mean_side = mean([row[1].size[0] for row in faces])   \n",
    "    number_of_faces = len(faces)\n",
    "    return [identity, mean_side, number_of_faces, faces]\n",
    "\n",
    "def get_sorted_identities(identities, discarded_faces, max_identities = 2, num_frames = 16):\n",
    "    sorted_identities = []\n",
    "    discarded_faces = []\n",
    "    for identity in identities:\n",
    "        sorted_identities.append(get_identity_information(identity, identities[identity]))\n",
    "\n",
    "\n",
    "    # Sort identities based on faces size\n",
    "    sorted_identities = sorted(sorted_identities, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    if len(sorted_identities) > max_identities:\n",
    "        sorted_identities = sorted_identities[:max_identities]\n",
    "\n",
    "    # Adjust the identities list faces number\n",
    "    identities_number = len(sorted_identities)\n",
    "    available_additional_faces = []\n",
    "    if identities_number > 1:\n",
    "        max_faces_per_identity = {1: [num_frames], \n",
    "                  2:  [int(num_frames/2), int(num_frames/2)],\n",
    "                  3:  [int(num_frames/3), int(num_frames/3), int(num_frames/4)],\n",
    "                  4:  [int(num_frames/3), int(num_frames/3), int(num_frames/8), int(num_frames/8)]}\n",
    "\n",
    "        max_faces_per_identity = max_faces_per_identity[identities_number]\n",
    "        for i in range(identities_number):\n",
    "            if sorted_identities[i][2] < max_faces_per_identity[i] and i < identities_number - 1:\n",
    "                sorted_identities[i+1][2] += max_faces_per_identity[i] - sorted_identities[i][2] \n",
    "                available_additional_faces.append(0)\n",
    "            elif sorted_identities[i][2] > max_faces_per_identity[i]:\n",
    "                available_additional_faces.append(sorted_identities[i][2] - max_faces_per_identity[i])\n",
    "                sorted_identities[i][2] = max_faces_per_identity[i]\n",
    "            else:\n",
    "                available_additional_faces.append(0)\n",
    "\n",
    "    else: # If only one identity is in the video, all the frames are assigned to this identity\n",
    "        sorted_identities[0][2] = num_frames\n",
    "        available_additional_faces.append(0)\n",
    "\n",
    "\n",
    "    # Check if we found enough faces to fullfill the input sequence, otherwise go back and add some faces from previous identities\n",
    "    input_sequence_length = sum(faces_number for _, _, faces_number, _ in sorted_identities)\n",
    "    if input_sequence_length < num_frames:\n",
    "        for i in range(identities_number):\n",
    "            needed_faces = num_frames - input_sequence_length\n",
    "            if available_additional_faces[i] > 0:\n",
    "                added_faces = min(available_additional_faces[i], needed_faces)\n",
    "                sorted_identities[i][2] += added_faces\n",
    "                input_sequence_length += added_faces\n",
    "                if input_sequence_length == num_frames:\n",
    "                    break\n",
    "        # If not enough faces have been found, add some \"dummy\" images in the last identity\n",
    "        if input_sequence_length < num_frames:\n",
    "            needed_faces = num_frames - input_sequence_length\n",
    "            sorted_identities[-1][2] += needed_faces\n",
    "            input_sequence_length += needed_faces\n",
    "    \n",
    "    return sorted_identities, discarded_faces\n",
    "\n",
    "def create_val_transform(size, additional_targets):\n",
    "    return Compose([\n",
    "        IsotropicResize(max_side=size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC),\n",
    "        PadIfNeeded(min_height=size, min_width=size, border_mode=cv2.BORDER_CONSTANT),\n",
    "        Resize(height=size, width=size)\n",
    "    ],  additional_targets = additional_targets, is_check_shapes= False\n",
    "    )\n",
    "\n",
    "def generate_masks(video_path, identities, discarded_faces, num_frames, image_size, num_patches):\n",
    "    mask = []\n",
    "    last_range_end = 0\n",
    "    sequence = []\n",
    "    size_embeddings = []\n",
    "    \n",
    "    images_frames = []\n",
    "    for identity_index, identity in enumerate(identities):\n",
    "        max_faces = identity[2]\n",
    "        identity_images = identity[3]\n",
    "        \n",
    "\n",
    "        # Select uniformly the frames in an alternate way\n",
    "        if len(identity_images) > max_faces:\n",
    "            idx = np.round(np.linspace(0, len(identity_images) - 2, max_faces)).astype(int)\n",
    "            images = []\n",
    "            for i in idx:\n",
    "                images.append(identity_images[i])\n",
    "            identity_images = images\n",
    "            \n",
    "        images_frames.extend(identity_image[0] for identity_image in identity_images)\n",
    "        identity_images = [identity_image[1] for identity_image in identity_images]\n",
    "\n",
    "        # Generate size embeddings\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "        width  = capture.get(3)  \n",
    "        height = capture.get(4) \n",
    "        video_area = width*height/2\n",
    "        identity_size_embeddings = []\n",
    "        \n",
    "        for image_index, image in enumerate(identity_images):\n",
    "            # Get face-frame area ratio for size embedding\n",
    "            face_area = image.size[0] * image.size[1]\n",
    "            ratio = int(face_area * 100 / video_area)\n",
    "            side_ranges = list(map(lambda a_: ratio in range(a_[0], a_[1] + 1), SIZE_EMB_DICT))\n",
    "            identity_size_embeddings.append(np.where(side_ranges)[0][0]+1)\n",
    "      \n",
    "\n",
    "        # If the readed faces are less than max_faces we need to add empty images and generate the mask\n",
    "        if len(identity_images) < max_faces: \n",
    "            diff = max_faces - len(identity_size_embeddings)\n",
    "            identity_size_embeddings = np.concatenate((identity_size_embeddings, np.zeros(diff)))\n",
    "            identity_images.extend([np.zeros((image_size, image_size, 3), dtype=np.uint8) for i in range(diff)])\n",
    "            mask.extend([1 if i < max_faces - diff else 0 for i in range(max_faces)])\n",
    "            images_frames.extend([max(images_frames) for i in range(diff)])\n",
    "        else: # Otherwise all the faces are valid\n",
    "            mask.extend([1 for i in range(max_faces)])\n",
    "\n",
    "        # Compose the size_embedding and sequence list\n",
    "        size_embeddings.extend(identity_size_embeddings)\n",
    "        sequence.extend(identity_images)\n",
    "\n",
    "    # Transform the images, the same transformation is applied to all the faces in the same video\n",
    "    sequence = [np.asarray(image) for image in sequence]\n",
    "    additional_targets_keys = [\"image\" + str(i) for i in range(num_frames)]\n",
    "    additional_targets_values = [\"image\" for i in range(num_frames)]\n",
    "    additional_targets = dict(zip(additional_targets_keys, additional_targets_values))\n",
    "\n",
    " \n",
    "    transform = create_val_transform(image_size, additional_targets)  \n",
    "    if len(sequence) == 8:\n",
    "        transformed_images = transform(image=sequence[0], image1=sequence[1], image2=sequence[2], image3=sequence[3], image4=sequence[4], image5=sequence[5], image6=sequence[6], image7=sequence[7])\n",
    "    elif len(sequence) == 16:\n",
    "        transformed_images = transform(image=sequence[0], image1=sequence[1], image2=sequence[2], image3=sequence[3], image4=sequence[4], image5=sequence[5], image6=sequence[6], image7=sequence[7], image8=sequence[8], image9=sequence[9], image10=sequence[10], image11=sequence[11], image12=sequence[12], image13=sequence[13], image14=sequence[14], image15=sequence[15])\n",
    "    else:\n",
    "        raise Exception(\"Invalid number of frames.\")\n",
    "\n",
    "    sequence = [transformed_images[key] for key in transformed_images]\n",
    "        \n",
    "    # Generate the identities_mask telling to the model which faces attend to an identity and which to another one\n",
    "    identities_mask = []\n",
    "    last_range_end = 0\n",
    "    for identity_index in range(len(identities)):\n",
    "        identity_mask = [True if i >= last_range_end and i < last_range_end + identities[identity_index][2] else False for i in range(0, num_frames)]\n",
    "        for k in range(identities[identity_index][2]):\n",
    "            identities_mask.append(identity_mask)\n",
    "        last_range_end += identities[identity_index][2]\n",
    "\n",
    "    # Generate coherent temporal-positional embedding\n",
    "    images_frames_positions = {k: v+1 for v, k in enumerate(sorted(set(images_frames)))}\n",
    "    frame_positions = [images_frames_positions[frame] for frame in images_frames]   \n",
    "    if num_patches != None: \n",
    "        positions = [[i+1 for i in range(((frame_position-1)*num_patches), num_patches*(frame_position))] for frame_position in frame_positions]\n",
    "        positions = sum(positions, []) # Merge the lists\n",
    "        positions.insert(0,0) # Add CLS\n",
    "    else:\n",
    "        positions = []\n",
    "\n",
    "    tokens_per_identity = [(identities[i][0], identities[i][2]*num_patches + identities[i-1][2]*num_patches) if i > 0 else (identities[i][0], identities[i][2]*num_patches) for i in range(len(identities))]     \n",
    "\n",
    "    return torch.tensor([sequence]).float(), torch.tensor([size_embeddings]).int(), torch.tensor([mask]).bool(), torch.tensor([identities_mask]).bool(), torch.tensor([positions]), tokens_per_identity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detect Faces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x\n",
    "\n",
    "def detect_faces(video_path):\n",
    "    # Init the face detector\n",
    "    detector = face_detector.FacenetDetector(device=\"cpu\")\n",
    "\n",
    "    # Read the video and its information\n",
    "    dataset = VideoDataset([video_path])\n",
    "    loader = DataLoader(dataset, shuffle=False, num_workers=0, batch_size=1, collate_fn= collate_fn)\n",
    "    \n",
    "    # Detect the faces\n",
    "    for item in loader: \n",
    "        bboxes = {}\n",
    "        video, indices, fps, frames = item[0]\n",
    "        bboxes.update({i : b for i, b in zip(indices, detector._detect_faces(frames))})\n",
    "        found_faces = False\n",
    "        for key in list(bboxes.keys()):\n",
    "            if type(bboxes[key]) == list:\n",
    "                found_faces = True\n",
    "                break\n",
    "\n",
    "        if not found_faces:\n",
    "            raise Exception(\"No faces found.\")\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crop Faces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crops(video_path, bboxes_dict):\n",
    "\n",
    "    # Read video frames\n",
    "    frames = []\n",
    "    \n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    frames_num = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(capture.get(5))\n",
    "\n",
    "    for i in range(frames_num):\n",
    "        capture.grab()\n",
    "        success, frame = capture.retrieve()\n",
    "        if not success:\n",
    "            continue\n",
    "        frames.append(frame)\n",
    "\n",
    "    # Extract the faces crops\n",
    "    explored_indexes = []\n",
    "    crops = []\n",
    "\n",
    "    for i in range(0, len(frames), fps):\n",
    "        while str(i) not in bboxes_dict:\n",
    "            if i == frames_num - 1:\n",
    "                i -= 1\n",
    "            if i in explored_indexes:\n",
    "                break\n",
    "            else:\n",
    "                explored_indexes.append(i)\n",
    "\n",
    "        frame = frames[i]\n",
    "        index = i\n",
    "        limit = i + fps - 1\n",
    "        keys = [int(x) for x in list(bboxes_dict.keys())]\n",
    "\n",
    "        while index < limit:\n",
    "            index += 1\n",
    "            if index in keys and bboxes_dict[index] is not None:\n",
    "                break\n",
    "        if index == limit:\n",
    "            continue\n",
    "\n",
    "        bboxes = bboxes_dict[index]\n",
    "        for bbox in bboxes:\n",
    "            xmin, ymin, xmax, ymax = [int(b * 2) for b in bbox]\n",
    "            w = xmax - xmin\n",
    "            h = ymax - ymin\n",
    "\n",
    "            # Add some padding to catch background too\n",
    "            p_h = h // 3\n",
    "            p_w = w // 3\n",
    "            \n",
    "            crop_h = (ymax + p_h) - max(ymin - p_h, 0)\n",
    "            crop_w = (xmax + p_w) - max(xmin - p_w, 0)\n",
    "\n",
    "            # Make the image square\n",
    "            if crop_h > crop_w:\n",
    "                p_h -= int(((crop_h - crop_w)/2))\n",
    "            else:\n",
    "                p_w -= int(((crop_w - crop_h)/2))\n",
    "\n",
    "            # Extract the face from the frame\n",
    "            crop = frame[max(ymin - p_h, 0):ymax + p_h, max(xmin - p_w, 0):xmax + p_w]\n",
    "            \n",
    "            # Check if out of bound and correct\n",
    "            h, w = crop.shape[:2]\n",
    "            if h > w:\n",
    "                diff = int((h - w)/2)\n",
    "                if diff > 0:         \n",
    "                    crop = crop[diff:-diff,:]\n",
    "                else:\n",
    "                    crop = crop[1:,:]\n",
    "            elif h < w:\n",
    "                diff = int((w - h)/2)\n",
    "                if diff > 0:\n",
    "                    crop = crop[:,diff:-diff]\n",
    "                else:\n",
    "                    crop = crop[:,:-1]\n",
    "\n",
    "            # Add the extracted face to the list\n",
    "            crops.append((i, Image.fromarray(crop), bbox))\n",
    "\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_faces(crops, valid_cluster_size_ratio = 0.20, similarity_threshold = 0.80):\n",
    "\n",
    "    # Convert crops to PIL images\n",
    "    crops_images = [row[1] for row in crops]\n",
    "    \n",
    "    # Extract the embeddings\n",
    "    embeddings_extractor = InceptionResnetV1(pretrained='vggface2').eval().to(\"cuda\")\n",
    "    faces = [preprocess_images(face) for face in crops_images]\n",
    "    faces = np.stack([np.uint8(face) for face in faces])\n",
    "    faces = torch.as_tensor(faces)\n",
    "    faces = faces.permute(0, 3, 1, 2).float()\n",
    "    faces = fixed_image_standardization(faces)\n",
    "    face_recognition_input = faces\n",
    "    embeddings = []\n",
    "    embeddings = embeddings_extractor(face_recognition_input.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    # Cluste\n",
    "    similarities = np.dot(np.array(embeddings), np.array(embeddings).T)\n",
    "    \n",
    "    components = _generate_connected_components(\n",
    "        similarities, similarity_threshold=similarity_threshold\n",
    "    )\n",
    "    components = [sorted(component) for component in components]\n",
    "\n",
    "    clustered_faces = {}\n",
    "    for identity_index, component in enumerate(components):\n",
    "        for index, face_index in enumerate(component):\n",
    "            component[index] = crops[face_index]\n",
    "        \n",
    "        clustered_faces[identity_index] = component\n",
    "\n",
    "    return clustered_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(video_path, clustered_faces, config, model_weights,discarded_faces = None, save_attentions = True):\n",
    "\n",
    "   \n",
    "    features_extractor = xception(num_classes=1, pretrain_path=\"pretrained/MINTIME_XC_Extractor_checkpoint30\")\n",
    "\n",
    "\n",
    "    model = SizeInvariantTimeSformer(config=config, require_attention=True)\n",
    "    \n",
    "    \n",
    "    features_extractor = torch.nn.DataParallel(features_extractor)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    features_extractor = features_extractor.to(device)    \n",
    "    model = model.to(device)\n",
    "    features_extractor.eval()\n",
    "    model.eval()\n",
    "\n",
    "    if os.path.exists(model_weights):\n",
    "        model.load_state_dict(torch.load(model_weights))\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint loaded for the model.\")    \n",
    "    \n",
    "    identities, discarded_faces  = get_sorted_identities(clustered_faces, discarded_faces, len(clustered_faces))\n",
    "    videos, size_embeddings, mask, identities_mask, positions, tokens_per_identity = generate_masks(video_path, identities, discarded_faces, config[\"model\"][\"num-frames\"], config[\"model\"][\"image-size\"], config[\"model\"][\"num-patches\"])\n",
    "    b, f, h, w, c = videos.shape\n",
    "    videos = videos.to(device)    \n",
    "    identities_mask = identities_mask.to(device)\n",
    "    mask = mask.to(device)\n",
    "    positions = positions.to(device)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        video = rearrange(videos, \"b f h w c -> (b f) c h w\")\n",
    "        features = features_extractor(video)  \n",
    "\n",
    "        features = rearrange(features, '(b f) c h w -> b f c h w', b = b, f = f)   \n",
    "        test_pred, attentions = model(features, mask=mask, size_embedding=size_embeddings, identities_mask=identities_mask, positions=positions)\n",
    "        \n",
    "        identity_names = [row[0] for row in tokens_per_identity]\n",
    "        frames_per_identity = [int(row[1] / config[\"model\"][\"num-patches\"]) for row in tokens_per_identity]\n",
    "        \n",
    "        if save_attentions:\n",
    "            aggregated_attentions, identity_attentions = aggregate_attentions(attentions, config['model']['heads'], config['model']['num-frames'], frames_per_identity)\n",
    "            # save_attention_plots(aggregated_attentions, identity_names, frames_per_identity, config['model']['num-frames'], os.path.basename(video_path), out_pth)\n",
    "        else:\n",
    "            identity_attentions = []\n",
    "            aggregated_attentions = []\n",
    "        return torch.sigmoid(test_pred[0]).item(), identity_attentions, aggregated_attentions, identities, frames_per_identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces(frame, bboxes):\n",
    "    xmin, ymin, xmax, ymax = [int(b * 2) for b in bboxes]\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "\n",
    "    # Add some padding to catch background too\n",
    "    p_h = h // 3\n",
    "    p_w = w // 3\n",
    "    \n",
    "    crop_h = (ymax + p_h) - max(ymin - p_h, 0)\n",
    "    crop_w = (xmax + p_w) - max(xmin - p_w, 0)\n",
    "\n",
    "    # Make the image square\n",
    "    if crop_h > crop_w:\n",
    "        p_h -= int(((crop_h - crop_w)/2))\n",
    "    else:\n",
    "        p_w -= int(((crop_w - crop_h)/2))\n",
    "\n",
    "    # Extract the face from the frame\n",
    "    crop = frame[max(ymin - p_h, 0):ymax + p_h, max(xmin - p_w, 0):xmax + p_w]\n",
    "    \n",
    "    # Check if out of bound and correct\n",
    "    h, w = crop.shape[:2]\n",
    "    if h > w:\n",
    "        diff = int((h - w)/2)\n",
    "        if diff > 0:         \n",
    "            crop = crop[diff:-diff,:]\n",
    "        else:\n",
    "            crop = crop[1:,:]\n",
    "    elif h < w:\n",
    "        diff = int((w - h)/2)\n",
    "        if diff > 0:\n",
    "            crop = crop[:,diff:-diff]\n",
    "        else:\n",
    "            crop = crop[:,:-1]\n",
    "    return crop\n",
    "\n",
    "def get_identities_bboxes(identities):\n",
    "    identities_bboxes = {}\n",
    "    for row in identities:\n",
    "        identity = row[3]\n",
    "        for face in identity:\n",
    "            frame = face[0]\n",
    "            if frame in identities_bboxes:\n",
    "                identities_bboxes[frame].append(face[2])\n",
    "            else:\n",
    "                identities_bboxes[frame] = [face[2]]\n",
    "    return identities_bboxes\n",
    "\n",
    "\n",
    "def generate_output_video(video_path, pred, identity_attentions, aggregated_attentions, identities, frames_per_identity, video_dir = \"\"):\n",
    "\n",
    "    identities_bboxes = get_identities_bboxes(identities)\n",
    "    available_frames_keys = [frame for frame in identities_bboxes]\n",
    "    path = os.path.join(\"Results1\",video_dir,os.path.basename(video_path).replace(\".mp4\",\"s\"))\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width  = cap.get(3)  \n",
    "    height = cap.get(4) \n",
    "    fps = int(cap.get(5))\n",
    "    fourcc = hex(int(cap.get(cv2.CAP_PROP_FOURCC)))\n",
    "    output = cv2.VideoWriter(\"examples/preds/\"+str(os.path.basename(video_path).replace(\".mp4\", \".avi\")), cv2.VideoWriter_fourcc(\"X\", \"V\", \"I\", \"D\"), fps, (int(width), int(height)))\n",
    "    frame_index = 0\n",
    "    while True:\n",
    "        config_gg = {}\n",
    "        ret, frame = cap.read()\n",
    "        config_gg[\"frame_number\"] = frame_index\n",
    "        config_gg[\"identities\"] = {}\n",
    "      \n",
    "        if ret:\n",
    "            nearest_frame_index = min(available_frames_keys, key=lambda x:abs(x - frame_index))\n",
    "            # if nearest_frame_index - frame_index > fps: \n",
    "            #     continue\n",
    "            \n",
    "\n",
    "            bbox = identities_bboxes[nearest_frame_index]\n",
    "            for identity_index, identity_bbox in enumerate(bbox):\n",
    "                identity_path = os.path.join(path,f\"frame_{frame_index}\",f\"identity_{identity_index}\")\n",
    "                if not os.path.exists(identity_path):\n",
    "                    os.makedirs(identity_path)\n",
    "                config_gg[\"identities\"][identity_index] = {}\n",
    "                config_gg[\"identities\"][identity_index][\"bbox\"] = identity_bbox \n",
    "                img = extract_faces(frame, identity_bbox)\n",
    "                xmin, ymin, xmax, ymax = [int(b * 2) for b in identity_bbox]\n",
    "                if pred > 0.5:\n",
    "                    red = 255 * identity_attentions[identity_index]\n",
    "                    green = 255 - red\n",
    "\n",
    "                    if red > green:\n",
    "                        text = 'Fake' + str(round(pred*100,2)) + \"%\"\n",
    "                    else:\n",
    "                        text = 'Pristine'\n",
    "                    config_gg[\"identities\"][identity_index][\"pred_result\"] = round(pred*100,2)\n",
    "                    config_gg[\"identities\"][identity_index][\"pred_label\"] =  'Fake' if 'Fake' in text else 'Pristine'\n",
    "                        \n",
    "                else:\n",
    "                    green = int(255 * (1 - pred))\n",
    "                    red = 255 - green\n",
    "                    text = 'Pristine '  + str(round((1-pred)*100,2)) + \"%\"\n",
    "                    config_gg[\"identities\"][identity_index][\"pred_result\"] = round(pred*100,2)\n",
    "                    config_gg[\"identities\"][identity_index][\"pred_label\"] =  'Pristine'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                cv2.imwrite(os.path.join(identity_path,f\"{uuid.uuid4()}.jpg\"), img)\n",
    "            \n",
    "                color = (0, green, red)\n",
    "                frame = draw_border(frame, (xmin,ymin), (xmax,ymax), color, 2, 10, 20)\n",
    "                cv2.putText(frame, text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                \n",
    "                # config_gg[\"identities\"][identity_index][\"img\"] = frame\n",
    "\n",
    "            config_gg[\"frame_path\"] = os.path.join(path, f\"frame_{frame_index}\", f\"{frame_index}.jpg\")\n",
    "            cv2.imwrite(config_gg[\"frame_path\"], frame)\n",
    "            with open(os.path.join(path, f\"frame_{frame_index}\",f\"{frame_index}.json\"), \"w\") as f:\n",
    "                json.dump(config_gg, f, indent = 6)\n",
    "            output.write(frame)        \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        frame_index += 1\n",
    "    output.release()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS IS THE MAIN TESTING FUNCTION**\n",
    "\n",
    "IT SAVES THE VIDEO RESULT TO AN OUTPUT FOLDER NAMED \"examples/preds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes of list of videos as input... to test single videos just give the input as `[video_path]` as in `save_config([\"path/to/video.mp4\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(vid):\n",
    "    count = 0\n",
    "    for video_path in vid:\n",
    "        print(video_path)\n",
    "        bboxes_dict = detect_faces(video_path)\n",
    "        print(\"Face detection completed.\")\n",
    "        \n",
    "        \n",
    "        print(\"Cropping faces from the video...\")\n",
    "        crops = extract_crops(video_path, bboxes_dict)\n",
    "        print(\"Faces cropping completed.\")\n",
    "        \n",
    "        \n",
    "        print(\"Clustering faces...\")\n",
    "        clustered_faces = cluster_faces(crops)\n",
    "        print(\"Faces clustering completed.\")\n",
    "\n",
    "        print(\"Searching for fakes in the video...\")\n",
    "        pred, identity_attentions, aggregated_attentions, identities, frames_per_identity = predict(video_path, clustered_faces, config, model_path)\n",
    "        if pred > 0.5:\n",
    "            print(\"The video is fake (\"+str(round(pred*100,2)) + \"%), showing video result...\")\n",
    "        else:\n",
    "            print(\"The video is pristine (\"+str(round((1-pred)*100,2)) + \"%), showing video result...\")\n",
    "\n",
    "        generate_output_video(video_path,pred,identity_attentions, aggregated_attentions, identities, frames_per_identity, f\"vid{count}\")\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = [\"your/video/path/here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000000.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207281/386711625.py:178: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987290837/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  return torch.tensor([sequence]).float(), torch.tensor([size_embeddings]).int(), torch.tensor([mask]).bool(), torch.tensor([identities_mask]).bool(), torch.tensor([positions]), tokens_per_identity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is pristine (91.84%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000002.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (96.16%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000009.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (94.81%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000012.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (98.54%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000023.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (94.85%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000015.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (97.64%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000010.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (99.92%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000017.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (96.0%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000005.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (98.83%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/source/train/train_00000011.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (82.12%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000007.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (98.37%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000005.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (100.0%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000003.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (92.07%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000007.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (98.37%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000012.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (98.04%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000014.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (62.96%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000019.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (87.25%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000009.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (98.43%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000015.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is pristine (92.57%), showing video result...\n",
      "VIDEOS/FFIW10K-v1/FFIW10K-v1-release/target/train/train_00000011.mp4\n",
      "Face detection completed.\n",
      "Cropping faces from the video...\n",
      "Faces cropping completed.\n",
      "Clustering faces...\n",
      "Faces clustering completed.\n",
      "Searching for fakes in the video...\n",
      "Features Extractor checkpoint loaded.\n",
      "The video is fake (50.69%), showing video result...\n"
     ]
    }
   ],
   "source": [
    "save_config(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
